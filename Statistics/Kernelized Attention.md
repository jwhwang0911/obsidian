---
cssclasses:
  - width-100
  - col-lines
  - row-alt
  - row-lines
  - table-small
tags:
  - Attention
---
## Preliminary - ê¸°ì¡´ì˜ Attention
$$  Attention(Q, K, V) = softmax(\frac {QK^T}{\sqrt{d_k}}) \cdot V $$
- Basic Attention : $softmax$ë¥¼ ì‚¬ìš©
	 Value ì•ì— ë¶™ëŠ” ë¶€ë¶„ => Weight, ë”°ë¼ì„œ weighted sumì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆìŒ.
	 Constriant : $softmax$ë•Œë¬¸ì— ì–‘ì˜ weightë“¤ë§Œ ì ìš© ê°€ëŠ¥
- Linear Attention


## ğŸ“cosFormer : Rethinking SOFTMAX in Attention
### 1.1 Analysis of SOFTMAX Attention

